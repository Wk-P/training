{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openpyxl import Workbook, load_workbook  # type: ignore\n",
    "import os\n",
    "\n",
    "\n",
    "def to_excel(\n",
    "    data, filename, sheet_style, result_dir_path, new_sheet=False, sheet_name=\"Sheet1\"\n",
    "):\n",
    "\n",
    "    if not os.path.exists(result_dir_path):\n",
    "        os.makedirs(result_dir_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(result_dir_path, f\"{filename}.xlsx\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        workbook = load_workbook(file_path)\n",
    "        if new_sheet:\n",
    "            sheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "        else:\n",
    "            sheet = workbook.active\n",
    "    else:\n",
    "        workbook = Workbook()\n",
    "        default_sheet = workbook.active\n",
    "        workbook.remove(default_sheet)\n",
    "        sheet = workbook.create_sheet(title=sheet_name)\n",
    "        \n",
    "    if sheet_style:\n",
    "        sheet.append(sheet_style)\n",
    "\n",
    "\n",
    "    if new_sheet:\n",
    "        print(data)\n",
    "    # write data into file\n",
    "    for row in data:\n",
    "        sheet.append(row)\n",
    "\n",
    "    workbook.save(filename=f\"{result_dir_path}\\\\{filename}.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "# read input dataset\n",
    "filename_prefix = (\n",
    "    \"RANDclient_test-DTWedSep251926542024\"\n",
    ")\n",
    "\n",
    "\n",
    "dataset_read_filename = filename_prefix\n",
    "training_data_dir = Path.cwd().parent / \"training_data\"\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "file_path = f\"{training_data_dir}\\\\{dataset_read_filename}.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "# set result output filename and path\n",
    "result_suffix = \"result\"\n",
    "\n",
    "result_dir_path = Path.cwd().parent / \"results\" / \"result_processTime_waitTasks_v3\"\n",
    "\n",
    "if not os.path.exists(result_dir_path):\n",
    "    os.makedirs(result_dir_path)\n",
    "\n",
    "version_index = len([_ for _ in Path(result_dir_path).iterdir() if _.is_file()])\n",
    "version = f\"_v{version_index}\"\n",
    "\n",
    "result_name = \"processTime#waitTasks\" + version\n",
    "result_output_filename = f\"{filename_prefix}{result_name}{result_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    columns = df.columns.to_list()\n",
    "    data_dict = {col: df[col].to_list() for col in columns}\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def data_preprocess(filepath):\n",
    "    data = read_data(filepath)\n",
    "    # TODO more...\n",
    "\n",
    "    # to numpy\n",
    "    for key in data.keys():\n",
    "        data[key] = np.array(data[key])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_preprocess(file_path)\n",
    "\n",
    "# if wait time < 1 => no wait [ remove ]\n",
    "is_remove_no_wait = False\n",
    "\n",
    "if is_remove_no_wait:\n",
    "    index_list = list()\n",
    "    for key, value in dataset.items():\n",
    "        for v in value:\n",
    "            if key == 'worker_wait_time' and v < 1:\n",
    "                index_list.append(list(dataset.get(key)).index(v))\n",
    "\n",
    "    for key, value in dataset.items():\n",
    "        dataset[key] = [x for i, x in enumerate(value) if i not in index_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Style View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# # dataset is a dictionary\n",
    "\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import cupy as cp                                      # type: ignore\n",
    "\n",
    "y = cp.array(dataset.get('worker_wait_time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strandard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler       # type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler         # type: ignore\n",
    "from sklearn.preprocessing import PowerTransformer     # type: ignore\n",
    "import numpy as np                                     # type: ignore\n",
    "\n",
    "_X = [dataset.get(\"request_num\"), dataset.get(\n",
    "    \"jobs_on_worker_node\")]\n",
    "\n",
    "for i in range(len(_X)):\n",
    "    _X[i] = np.array(_X[i]).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "# _scaler = \"normalization\"\n",
    "_scaler = None\n",
    "\n",
    "if _scaler == \"standard\":\n",
    "    X_scaler = StandardScaler()\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "elif _scaler == \"normalization\":\n",
    "    X_scaler = MinMaxScaler()\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "elif _scaler == \"log\":\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = np.log1p(_X[i])\n",
    "elif _scaler == \"power\":\n",
    "    X_scaler = PowerTransformer(method='yeo-johnson')\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "else:\n",
    "    pass\n",
    "\n",
    "for i in range(len(_X)):\n",
    "    _X[i] = _X[i].reshape(-1)\n",
    "\n",
    "X = cp.asarray(np.array(\n",
    "    _X\n",
    ").T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter combinations: 36\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the parameter ranges\n",
    "params = {\n",
    "    \"max_depth\": [3, 4, 5, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"n_estimators\": [1000, 1500, 2000],\n",
    "    \"min_child_weight\": [1],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"reg_alpha\": [1],\n",
    "    \"reg_lambda\": [1],\n",
    "}\n",
    "\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "parameter_combinations = [\n",
    "    dict(zip(params.keys(), combination))\n",
    "    for combination in itertools.product(*params.values())\n",
    "]\n",
    "\n",
    "# Print the number of parameter combinations and a few examples\n",
    "print(f\"Number of parameter combinations: {len(parameter_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor  # type: ignore\n",
    "import xgboost  # type: ignore\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error  # type: ignore\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# Create the XGBoost regressor model\n",
    "def create_model(params):\n",
    "    # if use gpu to add these two params\n",
    "    # - tree_method=\"hist\",\n",
    "    # - device=\"cuda\",\n",
    "    \n",
    "    return XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        reg_alpha=params[\"reg_alpha\"],\n",
    "        reg_lambda=params[\"reg_lambda\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(\n",
    "    params, X_train, y_train, X_test: np.array, y_test: np.array\n",
    ") -> Tuple[np.float64, xgboost.Booster]:\n",
    "\n",
    "    model = create_model(params)\n",
    "\n",
    "    # Convert CuPy arrays to NumPy arrays\n",
    "\n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "    # typecasting  cp -> np\n",
    "    y_test_np = cp.asnumpy(y_test)\n",
    "    X_test_np = cp.asnumpy(X_test)\n",
    "    predictions_np = cp.asnumpy(predictions)\n",
    "\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = float(mean_squared_error(y_test_np, predictions_np))\n",
    "    mae = float(mean_absolute_error(y_test_np, predictions_np))\n",
    "    avg = float(np.mean(y_test_np))\n",
    "\n",
    "    # calculate diffirence and accuracy\n",
    "    data_list = list()\n",
    "    for i in range(len(y_test_np)):\n",
    "        acc = 0\n",
    "        diff = abs(y_test_np[i] - predictions_np[i])\n",
    "        if predictions[i] < 0:\n",
    "            acc = 0\n",
    "        else:\n",
    "            rate = diff / y_test_np[i]\n",
    "            if rate < 1 and rate >= 0:\n",
    "                acc = 1 - rate\n",
    "            if rate > 1:\n",
    "                acc = 0\n",
    "\n",
    "        data_list.append(\n",
    "            [\n",
    "                float(X_test_np[i][0]),\n",
    "                float(X_test_np[i][1]),\n",
    "                float(y_test_np[i]),\n",
    "                float(predictions_np[i]),\n",
    "                float(diff),\n",
    "                float(round(acc, 5)),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    results_tuple = {\"mse\": mse, \"mae\": mae, \"avg\": avg, \"model\": model, \"data_list\": data_list}\n",
    "    filtered_results = {k: v for k, v in results_tuple.items() if k != \"model\" and k != \"data_list\"}\n",
    "    print(filtered_results, f'\\n{params}', '\\n')\n",
    "    return results_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\model_fit\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:28:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cpu, while the input data is on: cuda:0.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 171.35885687357785, 'mae': 10.103188562393186, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 165.94528086281687, 'mae': 10.042177194952963, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 165.65297912703707, 'mae': 10.159489743709562, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 177.43116167641568, 'mae': 10.610329635143279, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 192.69013793036729, 'mae': 11.0884819483757, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 206.06042923126105, 'mae': 11.536793482303617, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 209.15971246028062, 'mae': 11.712087116241452, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 232.81332179024645, 'mae': 12.347091497182843, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 253.05708975561504, 'mae': 12.866197285652156, 'avg': 149.7200319290161} \n",
      "{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 196.22910638290108, 'mae': 10.867705901861186, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 187.0985751569376, 'mae': 10.70551509618759, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 187.95939551436211, 'mae': 10.89347828388214, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 209.55682104314954, 'mae': 11.691592986583707, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 228.14828775493075, 'mae': 12.285340471267698, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 244.59158116493555, 'mae': 12.71342661499977, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 254.31658977143275, 'mae': 12.85451594829559, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 289.20770954519406, 'mae': 13.654004979133603, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 319.27939451745164, 'mae': 14.121148142814635, 'avg': 149.7200319290161} \n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 232.27813433155677, 'mae': 11.940809664726252, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 217.6399885389358, 'mae': 11.580822329521176, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 217.90977058645615, 'mae': 11.690131344795224, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 234.26471969075948, 'mae': 12.440263814926148, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 260.3323867588166, 'mae': 13.158595310449599, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 282.85342361405276, 'mae': 13.62733314990997, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 287.9580106751093, 'mae': 13.692474248409269, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 340.5955744904593, 'mae': 14.631505098342894, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 377.5521833328869, 'mae': 15.196155390739438, 'avg': 149.7200319290161} \n",
      "{'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 508.3027056056069, 'mae': 18.05327237606048, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 467.71663131333713, 'mae': 17.144423232078548, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 454.44970080922417, 'mae': 16.873266839981074, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 413.417063950002, 'mae': 16.26490495443344, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 445.4893348548609, 'mae': 16.81591854572296, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 474.12649446397097, 'mae': 17.268601124286647, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 467.0982721639274, 'mae': 17.25881603479385, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 513.0030531429229, 'mae': 17.950236556529994, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 543.379357070417, 'mae': 18.442400588989255, 'avg': 149.7200319290161} \n",
      "{'max_depth': 10, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "Best Parameters: {'max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1}\n",
      "Best MSE: 165.65297912703707\n",
      "Best MAE: 10.159489743709562\n",
      "Average: 149.7200319290161\n",
      "Results saved finished.\n",
      "[[370905.0, 185.0, 209.2069308757782, 211.79151916503906, 2.5845882892608643, 0.98765], [308447.0, 41.0, 28.37110447883606, 48.042057037353516, 19.670952558517456, 0.30666], [183590.0, 193.0, 205.2394654750824, 226.1626434326172, 20.92317795753479, 0.89805], [236519.0, 77.0, 76.19845652580261, 69.36711120605469, 6.831345319747925, 0.91035], [245356.0, 57.0, 47.94756126403809, 63.864749908447266, 15.917188644409173, 0.66803], [233951.0, 204.0, 218.4129946231842, 236.59994506835938, 18.18695044517517, 0.91673], [430853.0, 194.0, 227.217940568924, 224.30963134765625, 2.908309221267757, 0.9872], [255777.0, 66.0, 67.49503326416016, 63.98384475708008, 3.511188507080078, 0.94798], [142397.0, 38.0, 31.75894689559937, 31.539247512817383, 0.21969938278198597, 0.99308], [393502.0, 241.0, 274.2089517116547, 285.3021240234375, 11.09317231178278, 0.95954], [289541.0, 7.0, 12.1493501663208, 9.547399520874023, 2.6019506454467773, 0.78584], [288353.0, 101.0, 107.5886745452881, 104.9070816040039, 2.681592941284194, 0.97508], [383707.0, 214.0, 229.8588376045227, 246.6833953857422, 16.824557781219482, 0.9268], [387907.0, 50.0, 46.80675768852234, 51.41668701171875, 4.609929323196411, 0.90151], [245032.0, 191.0, 218.8765120506287, 219.68124389648438, 0.8047318458556845, 0.99632], [385186.0, 202.0, 211.0586938858032, 234.78155517578125, 23.722861289978056, 0.8876], [361435.0, 267.0, 301.3263144493103, 310.29534912109375, 8.969034671783447, 0.97023], [351076.0, 19.0, 23.85439777374268, 22.78105354309082, 1.073344230651859, 0.955], [115560.0, 163.0, 177.3936719894409, 177.37718200683594, 0.016489982604952047, 0.99991], [185878.0, 214.0, 234.5225930213928, 248.64639282226562, 14.123799800872831, 0.93978], [356875.0, 264.0, 298.7676420211792, 304.24249267578125, 5.474850654602051, 0.98168], [28985.0, 264.0, 319.4146063327789, 317.4449157714844, 1.9696905612944988, 0.99383], [487937.0, 143.0, 152.9946467876434, 163.24462890625, 10.249982118606596, 0.933], [497543.0, 181.0, 190.0809178352356, 214.6074676513672, 24.526549816131592, 0.87097], [318537.0, 44.0, 40.9799485206604, 48.47623062133789, 7.49628210067749, 0.81707], [155895.0, 246.0, 274.8764407634735, 272.8932189941406, 1.9832217693328857, 0.99279], [165218.0, 269.0, 320.1739640235901, 315.5346984863281, 4.639265537261963, 0.98551], [204382.0, 110.0, 119.6567404270172, 121.4388656616211, 1.782125234603896, 0.98511], [287964.0, 55.0, 45.75074291229248, 52.32631301879883, 6.575570106506348, 0.85627], [342881.0, 172.0, 180.3351321220398, 192.54080200195312, 12.20566987991333, 0.93232], [6164.0, 255.0, 315.279643535614, 308.52044677734375, 6.759196758270264, 0.97856], [412001.0, 171.0, 217.7059087753296, 197.7802276611328, 19.925681114196777, 0.90847], [163150.0, 235.0, 268.0604846477509, 270.7107238769531, 2.6502392292022137, 0.99011], [341265.0, 88.0, 81.65401339530945, 85.25566101074219, 3.6016476154327393, 0.95589], [89869.0, 3.0, 0.007497072219848633, -1.7310209274291992, 1.7385179996490479, 0.0], [163431.0, 171.0, 217.5614898204803, 199.22634887695312, 18.335140943527165, 0.91572], [293286.0, 215.0, 254.8379073143005, 247.59112548828125, 7.246781826019259, 0.97156], [199379.0, 40.0, 27.66329336166382, 38.56002426147461, 10.896730899810791, 0.60609], [401587.0, 109.0, 99.80891919136047, 125.94874572753906, 26.13982653617859, 0.7381], [71696.0, 37.0, 29.92891407012939, 38.72875213623047, 8.799838066101078, 0.70598], [433965.0, 198.0, 241.6720194816589, 227.1310272216797, 14.54099225997922, 0.93983], [31589.0, 53.0, 44.84964609146118, 56.166500091552734, 11.316854000091553, 0.74767], [484451.0, 262.0, 307.588919878006, 308.4170227050781, 0.8281028270721436, 0.99731], [453319.0, 94.0, 82.50860118865967, 103.46942901611328, 20.960827827453613, 0.74596], [210154.0, 21.0, 9.379688739776611, 26.608423233032227, 17.228734493255615, 0.0], [443625.0, 45.0, 38.57017207145691, 46.546512603759766, 7.9763405323028564, 0.7932], [293293.0, 1.0, 0.009074687957763672, 11.101949691772461, 11.092875003814697, 0.0], [54914.0, 9.0, 6.676593542098999, 12.987367630004883, 6.310774087905884, 0.05479], [147272.0, 220.0, 238.0752854347229, 237.16229248046875, 0.9129929542541504, 0.99617], [58806.0, 15.0, 5.999073266983032, 8.361889839172363, 2.362816572189331, 0.60614], [252010.0, 42.0, 37.53741025924683, 43.753170013427734, 6.215759754180901, 0.83441], [258211.0, 92.0, 93.0580506324768, 97.74620819091797, 4.688157558441162, 0.94962], [188905.0, 69.0, 86.94960117340088, 74.77477264404297, 12.17482852935791, 0.85998], [107371.0, 216.0, 250.4785239696503, 244.6265106201172, 5.8520133495331095, 0.97664], [153964.0, 100.0, 92.45864224433899, 97.4883041381836, 5.0296618938446045, 0.9456], [90288.0, 31.0, 32.73754596710205, 21.68359375, 11.05395221710205, 0.66235], [54654.0, 77.0, 51.77764821052551, 76.98767852783203, 25.21003031730652, 0.51311], [498461.0, 12.0, 12.42496156692505, 6.190892696380615, 6.234068870544435, 0.49826], [141087.0, 105.0, 127.1286029815674, 112.14849853515625, 14.980104446411147, 0.88217], [147905.0, 45.0, 44.25922203063965, 38.13344192504883, 6.12578010559082, 0.86159], [86976.0, 266.0, 320.9901256561279, 306.351806640625, 14.638319015502873, 0.9544], [470561.0, 138.0, 160.0550398826599, 161.0903778076172, 1.0353379249572754, 0.99353], [132659.0, 169.0, 177.6979601383209, 189.24664306640625, 11.548682928085356, 0.93501], [494065.0, 261.0, 289.1678237915039, 299.315673828125, 10.147850036621094, 0.96491], [44349.0, 46.0, 29.77317547798157, 47.993045806884766, 18.219870328903195, 0.38804], [204752.0, 24.0, 14.59258317947388, 22.79270362854004, 8.200120449066159, 0.43806], [232364.0, 241.0, 279.5606548786163, 284.198974609375, 4.638319730758724, 0.98341], [421726.0, 26.0, 33.88074946403503, 27.490339279174805, 6.390410184860222, 0.81139], [380913.0, 125.0, 135.4879062175751, 138.22450256347656, 2.736596345901461, 0.9798], [258455.0, 202.0, 242.3694312572479, 237.50616455078125, 4.863266706466646, 0.97993], [395641.0, 177.0, 228.0811095237732, 199.2560272216797, 28.825082302093506, 0.87362], [355324.0, 174.0, 227.0584681034088, 190.5203857421875, 36.53808236122131, 0.83908], [135441.0, 140.0, 171.3661842346191, 163.5663299560547, 7.799854278564425, 0.95448], [421399.0, 146.0, 185.9735045433044, 170.45803833007812, 15.515466213226262, 0.91657], [258422.0, 163.0, 171.3103861808777, 186.48834228515625, 15.177956104278536, 0.9114], [73085.0, 157.0, 174.6379146575928, 187.0708465576172, 12.432931900024386, 0.92881], [282270.0, 137.0, 161.182347536087, 159.7521209716797, 1.4302265644073202, 0.99113], [419832.0, 39.0, 32.63949680328369, 42.31024169921875, 9.670744895935059, 0.70371], [12098.0, 243.0, 271.7858250141144, 278.1951599121094, 6.409334897994995, 0.97642], [422682.0, 250.0, 285.1274428367615, 287.48358154296875, 2.3561387062072754, 0.99174], [23183.0, 78.0, 91.94563961029053, 71.5037612915039, 20.44187831878662, 0.77767], [244998.0, 49.0, 28.57234215736389, 46.81741714477539, 18.2450749874115, 0.36144], [33038.0, 256.0, 301.3835577964783, 314.09796142578125, 12.714403629302922, 0.95781], [177007.0, 87.0, 84.38933157920837, 84.39311981201172, 0.0037882328033447266, 0.99996], [459420.0, 167.0, 172.5193109512329, 190.9853057861328, 18.465994834899902, 0.89296], [372704.0, 52.0, 32.64063000679016, 44.15888214111328, 11.51825213432312, 0.64712], [476668.0, 93.0, 106.0722031593323, 97.63030242919922, 8.441900730133085, 0.92041], [214540.0, 219.0, 256.3508040904999, 247.03302001953125, 9.317784070968628, 0.96365], [323089.0, 161.0, 169.2796428203583, 177.4136505126953, 8.134007692337008, 0.95195], [20910.0, 141.0, 167.7634115219116, 169.54116821289062, 1.7777566909790323, 0.9894], [263876.0, 196.0, 206.5931534767151, 229.55587768554688, 22.962724208831787, 0.88885], [498492.0, 256.0, 295.5000464916229, 301.0115966796875, 5.511550188064575, 0.98135], [414667.0, 185.0, 196.7414879798889, 214.2506866455078, 17.509198665618925, 0.911], [92303.0, 169.0, 211.1295773983002, 196.8154754638672, 14.314101934433012, 0.9322], [199144.0, 231.0, 263.0796005725861, 252.90838623046875, 10.171214342117366, 0.96134], [154803.0, 177.0, 229.086199760437, 194.24105834960938, 34.84514141082764, 0.8479], [34825.0, 51.0, 59.39009571075439, 55.75672149658203, 3.633374214172356, 0.93882], [49188.0, 43.0, 31.45810675621033, 52.02373504638672, 20.565628290176388, 0.34625], [492077.0, 235.0, 261.1288108825684, 266.542236328125, 5.413425445556584, 0.97927], [61793.0, 10.0, 5.069785594940186, 3.0499446392059326, 2.019840955734254, 0.60159]]\n",
      "[[3, 0.01, 2000, 1, 0.7, 0.8, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track the best parameters\n",
    "best_params = None\n",
    "best_result = None\n",
    "best_data = None\n",
    "\n",
    "# results param\n",
    "best_mse = float(\"inf\")\n",
    "best_mae = float(\"inf\")\n",
    "\n",
    "sheet_style = [\"num\", \"jobs\", \"X_test\", \"prediction\", \"difference\", \"accuracy\"]\n",
    "\n",
    "# control sheet style of excel written\n",
    "first_write = True\n",
    "\n",
    "# Evaluate each parameter combination\n",
    "for params in parameter_combinations:\n",
    "\n",
    "    # split dataset to train dataset and test dataset for every params group\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    results_tuple = evaluate_model(params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    filtered_results = {\n",
    "        k: v for k, v in results_tuple.items() if k != \"model\" and k != \"data_list\"\n",
    "    }\n",
    "\n",
    "    # update best model\n",
    "    if results_tuple['mse'] < best_mse:\n",
    "        best_mse = results_tuple[\"mse\"]\n",
    "        best_params = params\n",
    "        best_model = results_tuple[\"model\"]\n",
    "        best_result = filtered_results\n",
    "        best_data = results_tuple['data_list']\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE:\", best_mse)\n",
    "print(\"Best MAE:\", best_result[\"mae\"])\n",
    "print(\"Average:\", best_result[\"avg\"])\n",
    "print(\"Results saved finished.\")\n",
    "\n",
    "params_keys_list = list(best_params.keys())\n",
    "params_values_list = list(best_params.values())\n",
    "\n",
    "to_excel(\n",
    "    data=best_data,\n",
    "    filename=result_output_filename,\n",
    "    sheet_style=sheet_style,\n",
    "    result_dir_path=result_dir_path,\n",
    "    new_sheet=True,  # Create a new sheet for the best results\n",
    ")\n",
    "\n",
    "# best mse params\n",
    "to_excel(\n",
    "    data=[params_values_list],\n",
    "    filename=result_output_filename,\n",
    "    sheet_style=params_keys_list,\n",
    "    result_dir_path=result_dir_path,\n",
    "    new_sheet=True,\n",
    "    sheet_name=\"best mse params\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_file = \"xgb_tasks_time_v1\" + \".json\"\n",
    "model_path = str(Path.cwd() / \"modelsfile\" / model_file)\n",
    "best_model.save_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
