{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openpyxl import Workbook, load_workbook  # type: ignore\n",
    "import os\n",
    "\n",
    "\n",
    "def to_excel(\n",
    "    data, filename, sheet_style, result_dir_path, new_sheet=False, sheet_name=\"Sheet1\"\n",
    "):\n",
    "\n",
    "    if not os.path.exists(result_dir_path):\n",
    "        os.makedirs(result_dir_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(result_dir_path, f\"{filename}.xlsx\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        workbook = load_workbook(file_path)\n",
    "        if new_sheet:\n",
    "            sheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "        else:\n",
    "            sheet = workbook.active\n",
    "    else:\n",
    "        workbook = Workbook()\n",
    "        default_sheet = workbook.active\n",
    "        workbook.remove(default_sheet)\n",
    "        sheet = workbook.create_sheet(title=sheet_name)\n",
    "        \n",
    "    if sheet_style:\n",
    "        sheet.append(sheet_style)\n",
    "\n",
    "\n",
    "    if new_sheet:\n",
    "        print(data)\n",
    "    # write data into file\n",
    "    for row in data:\n",
    "        sheet.append(row)\n",
    "\n",
    "    workbook.save(filename=f\"{result_dir_path}\\\\{filename}.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "# read input dataset\n",
    "filename_prefix = (\n",
    "    \"RANDclient_org-DTFriSep201819182024\"\n",
    ")\n",
    "\n",
    "\n",
    "dataset_read_filename = filename_prefix\n",
    "training_data_dir = Path.cwd().parent / \"training_data\"\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "file_path = f\"{training_data_dir}\\\\{dataset_read_filename}.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "# set result output filename and path\n",
    "result_suffix = \"result\"\n",
    "\n",
    "result_dir_path = Path.cwd().parent / \"results\" / \"result_processTime_waitTasks_v3\"\n",
    "\n",
    "if not os.path.exists(result_dir_path):\n",
    "    os.makedirs(result_dir_path)\n",
    "\n",
    "version_index = len([_ for _ in Path(result_dir_path).iterdir() if _.is_file()])\n",
    "version = f\"_v{version_index}\"\n",
    "\n",
    "result_name = \"processTime#waitTasks\" + version\n",
    "result_output_filename = f\"{filename_prefix}{result_name}{result_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    columns = df.columns.to_list()\n",
    "    data_dict = {col: df[col].to_list() for col in columns}\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def data_preprocess(filepath):\n",
    "    data = read_data(filepath)\n",
    "    # TODO more...\n",
    "\n",
    "    # to numpy\n",
    "    for key in data.keys():\n",
    "        data[key] = np.array(data[key])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_preprocess(file_path)\n",
    "\n",
    "# if wait time < 1 => no wait [ remove ]\n",
    "is_remove_no_wait = False\n",
    "\n",
    "if is_remove_no_wait:\n",
    "    index_list = list()\n",
    "    for key, value in dataset.items():\n",
    "        for v in value:\n",
    "            if key == 'worker_wait_time' and v < 1:\n",
    "                index_list.append(list(dataset.get(key)).index(v))\n",
    "\n",
    "    for key, value in dataset.items():\n",
    "        dataset[key] = [x for i, x in enumerate(value) if i not in index_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Style View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# # dataset is a dictionary\n",
    "\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import cupy as cp                                      # type: ignore\n",
    "\n",
    "y = cp.array(dataset.get('worker_wait_time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strandard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler       # type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler         # type: ignore\n",
    "from sklearn.preprocessing import PowerTransformer     # type: ignore\n",
    "import numpy as np                                     # type: ignore\n",
    "\n",
    "_X = [dataset.get(\"request_num\"), dataset.get(\n",
    "    \"jobs_on_worker_node\")]\n",
    "\n",
    "for i in range(len(_X)):\n",
    "    _X[i] = np.array(_X[i]).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "# _scaler = \"normalization\"\n",
    "_scaler = None\n",
    "\n",
    "if _scaler == \"standard\":\n",
    "    X_scaler = StandardScaler()\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "elif _scaler == \"normalization\":\n",
    "    X_scaler = MinMaxScaler()\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "elif _scaler == \"log\":\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = np.log1p(_X[i])\n",
    "elif _scaler == \"power\":\n",
    "    X_scaler = PowerTransformer(method='yeo-johnson')\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "else:\n",
    "    pass\n",
    "\n",
    "for i in range(len(_X)):\n",
    "    _X[i] = _X[i].reshape(-1)\n",
    "\n",
    "X = cp.asarray(np.array(\n",
    "    _X\n",
    ").T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter combinations: 36\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the parameter ranges\n",
    "params = {\n",
    "    \"max_depth\": [3, 4, 5, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"n_estimators\": [1000, 1500, 2000],\n",
    "    \"min_child_weight\": [1],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"reg_alpha\": [1],\n",
    "    \"reg_lambda\": [1],\n",
    "}\n",
    "\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "parameter_combinations = [\n",
    "    dict(zip(params.keys(), combination))\n",
    "    for combination in itertools.product(*params.values())\n",
    "]\n",
    "\n",
    "# Print the number of parameter combinations and a few examples\n",
    "print(f\"Number of parameter combinations: {len(parameter_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor  # type: ignore\n",
    "import xgboost  # type: ignore\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error  # type: ignore\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# Create the XGBoost regressor model\n",
    "def create_model(params):\n",
    "    # if use gpu to add these two params\n",
    "    # - tree_method=\"hist\",\n",
    "    # - device=\"cuda\",\n",
    "    \n",
    "    return XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        reg_alpha=params[\"reg_alpha\"],\n",
    "        reg_lambda=params[\"reg_lambda\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(\n",
    "    params, X_train, y_train, X_test: np.array, y_test: np.array\n",
    ") -> Tuple[np.float64, xgboost.Booster]:\n",
    "\n",
    "    model = create_model(params)\n",
    "\n",
    "    # Convert CuPy arrays to NumPy arrays\n",
    "\n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "    # typecasting  cp -> np\n",
    "    y_test_np = cp.asnumpy(y_test)\n",
    "    X_test_np = cp.asnumpy(X_test)\n",
    "    predictions_np = cp.asnumpy(predictions)\n",
    "\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = float(mean_squared_error(y_test_np, predictions_np))\n",
    "    mae = float(mean_absolute_error(y_test_np, predictions_np))\n",
    "    avg = float(np.mean(y_test_np))\n",
    "\n",
    "    # calculate diffirence and accuracy\n",
    "    data_list = list()\n",
    "    for i in range(len(y_test_np)):\n",
    "        acc = 0\n",
    "        diff = abs(y_test_np[i] - predictions_np[i])\n",
    "        if predictions[i] < 0:\n",
    "            acc = 0\n",
    "        else:\n",
    "            rate = diff / y_test_np[i]\n",
    "            if rate < 1 and rate >= 0:\n",
    "                acc = 1 - rate\n",
    "            if rate > 1:\n",
    "                acc = 0\n",
    "\n",
    "        data_list.append(\n",
    "            [\n",
    "                float(X_test_np[i][0]),\n",
    "                float(X_test_np[i][1]),\n",
    "                float(y_test_np[i]),\n",
    "                float(predictions_np[i]),\n",
    "                float(diff),\n",
    "                float(round(acc, 5)),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    results_tuple = {\"mse\": mse, \"mae\": mae, \"avg\": avg, \"model\": model, \"data_list\": data_list}\n",
    "    filtered_results = {k: v for k, v in results_tuple.items() if k != \"model\" and k != \"data_list\"}\n",
    "    print(filtered_results, f'\\n{params}', '\\n')\n",
    "    return results_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\model_fit\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:33:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cpu, while the input data is on: cuda:0.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 412.9010469870167, 'mae': 15.57434986114502, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 319.11034100696276, 'mae': 13.874605025053027, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 294.5835217963246, 'mae': 13.527389668226244, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 273.2705532287575, 'mae': 13.248723964691164, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 284.8928199346261, 'mae': 13.615240528583529, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 299.01721491130775, 'mae': 13.9540535736084, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 288.0732020851492, 'mae': 13.573653516769411, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 306.0196262831011, 'mae': 13.999647631645205, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 319.54179412451896, 'mae': 14.308765654563908, 'avg': 277.6025368595123} \n",
      "{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 571.692827880437, 'mae': 18.646355562210083, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 448.3105751547223, 'mae': 16.748898904323582, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 413.31056575669186, 'mae': 16.303601154983046, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 353.9202338346378, 'mae': 15.339656326770784, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 361.68733236081135, 'mae': 15.527649748325352, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 379.89952178468116, 'mae': 15.86714794158936, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 364.1850153302306, 'mae': 15.21547792553902, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 388.68870691429134, 'mae': 15.564056068062786, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 410.534829609653, 'mae': 16.042204169034964, 'avg': 277.6025368595123} \n",
      "{'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 801.4986890897784, 'mae': 22.501069846153264, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 638.816930950089, 'mae': 20.377934300899508, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 586.2829720966577, 'mae': 19.64866395711899, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 475.4961315671592, 'mae': 17.741370606422425, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 478.44321769271977, 'mae': 17.798541576862334, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 493.3399876565065, 'mae': 18.005216977596284, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 454.9227135067382, 'mae': 17.030379550457003, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 476.30834631862643, 'mae': 17.38397433280945, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 497.56154962457816, 'mae': 17.7141219329834, 'avg': 277.6025368595123} \n",
      "{'max_depth': 5, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 2233.0999876132996, 'mae': 38.57615413665772, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1948.305238022384, 'mae': 35.89131476402283, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1824.5345590267184, 'mae': 34.6647951078415, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1305.8677415879447, 'mae': 29.70108459889889, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1279.0884717675767, 'mae': 29.240788764953617, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1267.5611949292788, 'mae': 29.008756031990057, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1170.4368170038565, 'mae': 27.919525473117833, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1162.1800429629882, 'mae': 27.695741062164313, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.1, 'n_estimators': 1500, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "{'mse': 1159.9899786602189, 'mae': 27.627336158752446, 'avg': 277.6025368595123} \n",
      "{'max_depth': 10, 'learning_rate': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1} \n",
      "\n",
      "Best Parameters: {'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 1}\n",
      "Best MSE: 273.2705532287575\n",
      "Best MAE: 13.248723964691164\n",
      "Average: 277.6025368595123\n",
      "Results saved finished.\n",
      "[[205712.0, 353.0, 401.7542209625244, 383.5929260253906, 18.16129493713379, 0.9548], [58029.0, 73.0, 73.54018759727478, 85.29373931884766, 11.753551721572876, 0.84018], [340903.0, 366.0, 400.5878090858459, 418.2171936035156, 17.629384517669735, 0.95599], [467866.0, 155.0, 193.5551633834839, 175.0742950439453, 18.480868339538574, 0.90452], [242117.0, 104.0, 129.5101869106293, 114.17466735839844, 15.335519552230863, 0.88159], [63577.0, 386.0, 434.9069526195526, 426.53857421875, 8.368378400802612, 0.98076], [483028.0, 369.0, 405.3086099624634, 404.88311767578125, 0.4254922866821289, 0.99895], [72583.0, 124.0, 124.7193982601166, 144.5659637451172, 19.846565485000582, 0.84087], [434950.0, 68.0, 70.53611874580383, 70.71253967285156, 0.1764209270477295, 0.9975], [361825.0, 439.0, 477.4588196277618, 495.8567810058594, 18.39796137809759, 0.96147], [73653.0, 10.0, 7.991539001464844, 14.43994426727295, 6.4484052658081055, 0.1931], [179620.0, 193.0, 220.7374815940857, 237.38169860839844, 16.644217014312744, 0.9246], [22646.0, 397.0, 434.0687549114227, 463.9155578613281, 29.846802949905452, 0.93124], [234914.0, 84.0, 72.10030198097229, 77.70266723632812, 5.602365255355835, 0.9223], [145729.0, 363.0, 399.175333738327, 400.6103210449219, 1.4349873065948486, 0.99641], [440802.0, 380.0, 423.1519556045532, 410.2584228515625, 12.893532752990723, 0.96953], [109896.0, 483.0, 533.7881543636322, 529.6469116210938, 4.141242742538452, 0.99224], [145325.0, 31.0, 19.63588166236877, 20.293365478515625, 0.6574838161468541, 0.96652], [183516.0, 312.0, 347.9595651626587, 343.55084228515625, 4.408722877502441, 0.98733], [173418.0, 399.0, 448.8875997066498, 450.2333984375, 1.3457987308502197, 0.997], [112097.0, 478.0, 542.8030648231506, 521.5601806640625, 21.242884159088135, 0.96086], [133298.0, 479.0, 562.5451629161835, 525.6248779296875, 36.92028498649597, 0.93437], [474058.0, 276.0, 316.0495824813843, 293.7658386230469, 22.283743858337402, 0.92949], [72574.0, 349.0, 390.4993538856506, 396.3597106933594, 5.860356807708797, 0.98499], [76029.0, 76.0, 73.9533793926239, 99.0066909790039, 25.053311586380005, 0.66123], [35601.0, 449.0, 524.9288382530212, 496.852783203125, 28.07605504989624, 0.94651], [67146.0, 485.0, 569.7413742542267, 534.3666381835938, 35.374736070632935, 0.93791], [168858.0, 209.0, 258.6472954750061, 244.65191650390625, 13.995378971099854, 0.94589], [469957.0, 100.0, 85.49987864494324, 121.95061492919922, 36.45073628425598, 0.57367], [458145.0, 329.0, 361.8420634269714, 360.01239013671875, 1.8296732902526287, 0.99494], [447735.0, 463.0, 517.5753047466278, 528.5184326171875, 10.943127870559692, 0.97886], [355696.0, 331.0, 358.5753834247589, 359.158203125, 0.5828197002410889, 0.99837], [215621.0, 429.0, 485.1109266281128, 471.574951171875, 13.535975456237793, 0.9721], [79960.0, 173.0, 212.1433215141296, 188.22674560546875, 23.91657590866086, 0.88726], [331526.0, 3.0, 0.06816864013671875, -5.616828441619873, 5.684997081756592, 0.0], [47867.0, 328.0, 358.2513031959534, 358.20123291015625, 0.050070285797175984, 0.99986], [186149.0, 400.0, 436.0531215667725, 444.6123962402344, 8.559274673461857, 0.98037], [244731.0, 70.0, 70.5839295387268, 76.39151000976562, 5.807580471038818, 0.91772], [403343.0, 207.0, 217.6443274021149, 219.1202392578125, 1.4759118556976034, 0.99322], [287557.0, 63.0, 47.41866660118103, 65.93179321289062, 18.513126611709595, 0.60958], [18544.0, 376.0, 420.1930510997772, 418.1658935546875, 2.0271575450897217, 0.99518], [364200.0, 93.0, 76.22445178031921, 97.21309661865234, 20.98864483833313, 0.72465], [227178.0, 473.0, 552.281406879425, 524.5364990234375, 27.74490785598755, 0.94976], [465630.0, 184.0, 208.2626202106476, 204.69044494628906, 3.572175264358549, 0.98285], [15487.0, 34.0, 21.07099175453186, 39.93048095703125, 18.85948920249939, 0.10495], [415345.0, 77.0, 89.31005477905273, 108.92668914794922, 19.616634368896484, 0.78035], [82944.0, 1.0, 0.07079172134399414, -4.6683430671691895, 4.739134788513184, 0.0], [403134.0, 12.0, 8.460765361785889, 1.280120849609375, 7.180644512176514, 0.1513], [237606.0, 406.0, 443.4546165466309, 433.94342041015625, 9.511196136474666, 0.97855], [448264.0, 23.0, 26.05965495109558, 16.89908790588379, 9.160567045211792, 0.64848], [499719.0, 72.0, 56.18538641929626, 78.83702087402344, 22.65163445472718, 0.59684], [122840.0, 181.0, 207.1301114559174, 208.0910186767578, 0.9609072208404257, 0.99536], [278199.0, 131.0, 167.8152408599854, 132.799560546875, 35.01568031311041, 0.79134], [3847.0, 401.0, 453.9184086322784, 462.0100402832031, 8.09163165092474, 0.98217], [422404.0, 192.0, 198.2441132068634, 230.94468688964844, 32.700573682785034, 0.83505], [264135.0, 55.0, 48.94209599494934, 51.883052825927734, 2.9409568309783936, 0.93991], [48803.0, 148.0, 152.2193369865417, 164.55999755859375, 12.340660572052059, 0.91893], [7597.0, 19.0, 10.23706603050232, 20.644073486328125, 10.407007455825806, 0.0], [214265.0, 202.0, 226.4671356678009, 215.43531799316406, 11.03181767463684, 0.95129], [218710.0, 78.0, 69.6548867225647, 94.68965911865234, 25.034772396087646, 0.64059], [396656.0, 482.0, 563.8073768615723, 529.2567749023438, 34.550601959228516, 0.93872], [346456.0, 258.0, 292.2144539356232, 295.92193603515625, 3.707482099533024, 0.98731], [233806.0, 318.0, 349.8811328411102, 346.6345520019531, 3.2465808391570476, 0.99072], [241224.0, 471.0, 517.7467651367188, 541.4525146484375, 23.70574951171875, 0.95421], [60473.0, 79.0, 74.54515433311462, 92.106689453125, 17.561535120010376, 0.76442], [311583.0, 40.0, 27.07822322845459, 46.81474304199219, 19.736519813537598, 0.27113], [36684.0, 440.0, 520.3092715740204, 500.824951171875, 19.484320402145386, 0.96255], [351386.0, 46.0, 36.43178796768188, 38.75992965698242, 2.328141689300544, 0.9361], [150057.0, 235.0, 260.5393083095551, 255.91799926757812, 4.6213090419769856, 0.98226], [346207.0, 383.0, 430.0447556972504, 428.3252258300781, 1.719529867172298, 0.996], [15290.0, 345.0, 385.2446985244751, 388.28594970703125, 3.0412511825561523, 0.99211], [82281.0, 334.0, 373.6273846626282, 359.1091003417969, 14.518284320831299, 0.96114], [487576.0, 273.0, 308.4597790241241, 298.1590576171875, 10.300721406936589, 0.96661], [460258.0, 286.0, 307.5611889362335, 315.3357238769531, 7.7745349407196045, 0.97472], [327708.0, 313.0, 339.5384836196899, 344.82177734375, 5.283293724060115, 0.98444], [132471.0, 300.0, 335.394779920578, 337.1489562988281, 1.754176378250122, 0.99477], [25359.0, 264.0, 298.698920249939, 311.64678955078125, 12.947869300842228, 0.95665], [170147.0, 69.0, 54.44474315643311, 77.73698425292969, 23.292241096496575, 0.57219], [242604.0, 444.0, 499.9639925956726, 509.40777587890625, 9.443783283233643, 0.98111], [93397.0, 453.0, 511.2813062667847, 503.74871826171875, 7.532588005065975, 0.98527], [422465.0, 154.0, 153.5462865829468, 173.10499572753906, 19.558709144592257, 0.87262], [39699.0, 82.0, 74.507488489151, 91.77435302734375, 17.26686453819275, 0.76825], [87454.0, 465.0, 517.2816815376282, 534.17529296875, 16.893611431121826, 0.96734], [475466.0, 172.0, 186.7802538871765, 194.2321319580078, 7.451878070831299, 0.9601], [328372.0, 316.0, 349.2444701194763, 343.6298828125, 5.614587306976318, 0.98392], [7414.0, 90.0, 76.17507076263428, 101.74327850341797, 25.56820774078369, 0.66435], [305826.0, 179.0, 221.1133344173431, 196.81512451171875, 24.29820990562436, 0.89011], [193900.0, 405.0, 453.7782278060913, 447.97705078125, 5.801177024841309, 0.98722], [286996.0, 308.0, 346.0099620819092, 339.3081359863281, 6.701826095581055, 0.98063], [335253.0, 274.0, 286.2073707580566, 303.0202331542969, 16.81286239624029, 0.94126], [192478.0, 373.0, 418.1037108898163, 424.5077819824219, 6.404071092605591, 0.98468], [350731.0, 460.0, 512.6359791755676, 507.37957763671875, 5.256401538848877, 0.98975], [178348.0, 354.0, 392.4042451381683, 398.7728576660156, 6.368612527847347, 0.98377], [176362.0, 319.0, 353.7591283321381, 360.3907775878906, 6.631649255752507, 0.98125], [401883.0, 420.0, 469.7716851234436, 489.0039367675781, 19.23225164413452, 0.95906], [184882.0, 340.0, 379.8852272033691, 373.4422912597656, 6.442935943603459, 0.98304], [304444.0, 86.0, 104.7356345653534, 74.71131134033203, 30.024323225021362, 0.71333], [364331.0, 75.0, 64.4491708278656, 97.27050018310547, 32.82132935523987, 0.49074], [118286.0, 427.0, 458.4804861545563, 471.6392822265625, 13.158796072006226, 0.9713], [122257.0, 16.0, 9.060096502304077, 22.34186363220215, 13.281767129898071, 0.0]]\n",
      "[[3, 0.05, 1000, 1, 0.7, 0.8, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track the best parameters\n",
    "best_params = None\n",
    "best_result = None\n",
    "best_data = None\n",
    "\n",
    "# results param\n",
    "best_mse = float(\"inf\")\n",
    "best_mae = float(\"inf\")\n",
    "\n",
    "sheet_style = [\"num\", \"jobs\", \"X_test\", \"prediction\", \"difference\", \"accuracy\"]\n",
    "\n",
    "# control sheet style of excel written\n",
    "first_write = True\n",
    "\n",
    "# Evaluate each parameter combination\n",
    "for params in parameter_combinations:\n",
    "\n",
    "    # split dataset to train dataset and test dataset for every params group\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    results_tuple = evaluate_model(params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    filtered_results = {\n",
    "        k: v for k, v in results_tuple.items() if k != \"model\" and k != \"data_list\"\n",
    "    }\n",
    "\n",
    "    # update best model\n",
    "    if results_tuple['mse'] < best_mse:\n",
    "        best_mse = results_tuple[\"mse\"]\n",
    "        best_params = params\n",
    "        best_model = results_tuple[\"model\"]\n",
    "        best_result = filtered_results\n",
    "        best_data = results_tuple['data_list']\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE:\", best_mse)\n",
    "print(\"Best MAE:\", best_result[\"mae\"])\n",
    "print(\"Average:\", best_result[\"avg\"])\n",
    "print(\"Results saved finished.\")\n",
    "\n",
    "params_keys_list = list(best_params.keys())\n",
    "params_values_list = list(best_params.values())\n",
    "\n",
    "to_excel(\n",
    "    data=best_data,\n",
    "    filename=result_output_filename,\n",
    "    sheet_style=sheet_style,\n",
    "    result_dir_path=result_dir_path,\n",
    "    new_sheet=True,  # Create a new sheet for the best results\n",
    ")\n",
    "\n",
    "# best mse params\n",
    "to_excel(\n",
    "    data=[params_values_list],\n",
    "    filename=result_output_filename,\n",
    "    sheet_style=params_keys_list,\n",
    "    result_dir_path=result_dir_path,\n",
    "    new_sheet=True,\n",
    "    sheet_name=\"best mse params\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_file = \"xgb_tasks_time_fn\" + \".json\"\n",
    "model_path = str(Path.cwd() / \"modelsfile\" / model_file)\n",
    "best_model.save_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
