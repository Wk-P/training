{"cells":[{"cell_type":"markdown","metadata":{"id":"qgtH9_K1imY7"},"source":["### DATA PREPROGRESS"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":490,"status":"ok","timestamp":1715196153067,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"wHuwHZf_jw9N"},"outputs":[],"source":["import re\n","import numpy as np\n","import json\n","\n","def data_preprocess(filepath):\n","    # data_all = np.genfromtxt(filepath, delimiter=' ')\n","    # data = data_all[:8000]\n","\n","    data = np.genfromtxt(filepath, delimiter=' ', dtype=float)\n","\n","    for d in data:\n","        # d[0] = float(d[0] * 1000)\n","        # d[2] = float(d[2] * 1000)\n","        # d[3] = float(d[2] * 1000)\n","        # d[4] = float(d[2] * 1000)\n","        pass\n","\n","    training_x = data[:, 1:]\n","\n","    training_y = data[:, :1]\n","\n","    return training_x, training_y\n","\n","# Data preprocessing\n","file_path = \"../training_data/trainingDataSet_CPU_v1_10.txt\"\n","training_x, training_y = data_preprocess(file_path)"]},{"cell_type":"markdown","metadata":{"id":"ct0ytUeniiq0"},"source":["### MODEL COMPILE & FIT"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1715196153561,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"q0bCz2Ma4sQu"},"outputs":[],"source":["### Data preprocessing using MinMaxScaler\n","# create MinMaxScaler Object\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","x_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()\n","\n","\n","# scaled_training_x = training_x[:, 0].reshape(-1, 1)\n","# scaled_training_y = training_y[:].reshape(-1, 1)\n","\n","# training_x[:, 0] = scaled_training_x[:, 0]\n","# training_y = scaled_training_y\n","\n","# Define the split points based on time\n","train_size = int(len(training_x) * 0.8)\n","val_size = int(len(training_x) * 0.1)\n","\n","# Split the data\n","x_train = training_x[:train_size]\n","y_train = training_y[:train_size]\n","\n","x_val = training_x[train_size + val_size:]\n","y_val = training_y[train_size + val_size:]\n","\n","\n","# test data from other file\n","x_test = training_x[:train_size + val_size]\n","y_test = training_y[:train_size + val_size]\n","\n","# x_train_first_column = x_train[:, 0].reshape(-1, 1)\n","# scaled_x_train_first_column = x_scaler.fit_transform(x_train_first_column)\n","# x_train[:, 0] = scaled_x_train_first_column.flatten()\n","# y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))  # reshap to 2D\n","\n","# make the same sacling on valid data set and test data set\n","# x_val_scaled = x_scaler.transform(x_val)\n","# x_val_first_column = x_val[:, 0].reshape(-1, 1)\n","# scaled_x_val_first_column = x_scaler.fit_transform(x_val_first_column)\n","# x_val[:, 0] = scaled_x_val_first_column.flatten()\n","# y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1))  # reshap to 2D\n","\n","\n","# x_test_first_column = x_test[:, 0].reshape(-1, 1)\n","# scaled_x_test_first_column = x_scaler.fit_transform(x_test_first_column)\n","# x_test[:, 0] = scaled_x_test_first_column.flatten()\n","# y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))  # reshap to 2D"]},{"cell_type":"markdown","metadata":{"id":"B1eEvGJzL186"},"source":["### NORMALIZATION"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":342,"status":"ok","timestamp":1715196153902,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"VGTTcyJALYSX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout, Input\n","from keras.callbacks import Callback, EarlyStopping\n","from keras.optimizers import Adam\n","\n","\n","def create_sequences(dataset_x, dataset_y, time_steps):\n","    dataX, dataY = [], []\n","\n","    # scale for data set\n","    if dataset_x.shape[0] < time_steps:\n","        # Handle the case where the input data is smaller than time_steps\n","        # You can choose to pad the sequence or handle it in a way that makes sense for your data\n","        # For example, you can replicate the single time step to create a sequence\n","        x_sequence = np.tile(dataset_x, (time_steps, 1))\n","        y_sequence = np.tile(dataset_y, (time_steps, 1))\n","        dataX.append(x_sequence)\n","        dataY.append(y_sequence)\n","    else:\n","        for i in range(dataset_x.shape[0] - time_steps + 1):\n","            x_sequence = dataset_x[i:i + time_steps, :]\n","            y_sequence = dataset_y[i + time_steps - 1]\n","            dataX.append(x_sequence)\n","            dataY.append(y_sequence)\n","\n","    return np.array(dataX), np.array(dataY)\n","\n","# Create sequences for each set\n","time_steps = 50\n","\n","\n","# x_test = training_x[train_size:]\n","# y_test = one_hot_encoding[train_size:]\n","\n","# Create sequences for each set\n","x_train_sequences, y_train_sequences = create_sequences(x_train, y_train, time_steps)\n","x_val_sequences, y_val_sequences = create_sequences(x_val, y_val, time_steps)\n","x_test_sequences, y_test_sequences = create_sequences(x_test, y_test, time_steps)\n","\n","\n","# Initialize LossHistory with validation data\n","class LossHistory(Callback):\n","    def __init__(self, x_train, y_train, x_val, y_val):\n","        super().__init__()\n","        self.train_data = (x_train, y_train)\n","        self.validation_data = (x_val, y_val)\n","        self.losses = []\n","        self.val_losses = []\n","        self.train_errors = []\n","        self.val_errors = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.losses.append(logs['loss'])\n","        self.val_losses.append(logs.get('val_loss'))\n","        self.train_errors.append(logs.get('mean_absolute_error'))\n","        self.val_errors.append(logs.get('val_mean_absolute_error'))"]},{"cell_type":"markdown","metadata":{"id":"k_HW6eWMLZMX"},"source":["### FITTING"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":58616,"status":"error","timestamp":1715196212517,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"F3ZXcO-_j4eM","outputId":"f89bb4c1-94d0-4916-da3d-f77068c91b52"},"outputs":[{"name":"stdout","output_type":"stream","text":["epochs: 200\n","batch: 64\n","units: 50\n","hidden layers: 0\n","Epoch 1/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 36.3554 - mean_absolute_error: 4.2438 - val_loss: 32.0970 - val_mean_absolute_error: 4.2949\n","Epoch 2/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 32.6519 - mean_absolute_error: 4.1748 - val_loss: 31.9831 - val_mean_absolute_error: 4.1920\n","Epoch 3/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 32.0116 - mean_absolute_error: 4.1159 - val_loss: 31.9878 - val_mean_absolute_error: 4.2079\n","Epoch 4/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 32.0884 - mean_absolute_error: 4.1411 - val_loss: 32.1482 - val_mean_absolute_error: 4.3186\n","Epoch 5/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 31.9553 - mean_absolute_error: 4.1238 - val_loss: 32.0482 - val_mean_absolute_error: 4.2672\n","Epoch 6/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 32.2262 - mean_absolute_error: 4.1410 - val_loss: 32.2343 - val_mean_absolute_error: 4.0493\n","Epoch 7/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 31.9017 - mean_absolute_error: 4.1098 - val_loss: 32.3940 - val_mean_absolute_error: 4.4030\n","Epoch 8/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 33.1400 - mean_absolute_error: 4.2019 - val_loss: 32.0224 - val_mean_absolute_error: 4.1277\n","Epoch 9/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 32.1151 - mean_absolute_error: 4.1342 - val_loss: 31.9935 - val_mean_absolute_error: 4.1548\n","Epoch 10/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 32.6198 - mean_absolute_error: 4.1718 - val_loss: 31.9833 - val_mean_absolute_error: 4.1779\n","Epoch 11/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 33.0604 - mean_absolute_error: 4.1901 - val_loss: 32.0720 - val_mean_absolute_error: 4.2816\n","Epoch 12/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 32.3331 - mean_absolute_error: 4.1775 - val_loss: 32.1528 - val_mean_absolute_error: 4.3206\n","\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n","MAE: 4.269085338724534\n","pred: [5.8416357] | real: [12.07354832]\n","pred: [5.8416357] | real: [10.56662965]\n","pred: [5.8416357] | real: [15.8947897]\n","pred: [5.8416357] | real: [16.50529051]\n","pred: [5.8416357] | real: [16.00296664]\n","pred: [5.8416357] | real: [20.03378606]\n","pred: [5.8416357] | real: [13.99754524]\n","pred: [5.8416357] | real: [12.6154778]\n","pred: [5.8416357] | real: [5.98408818]\n","pred: [5.8416357] | real: [16.37700558]\n","pred: [5.8416357] | real: [7.95711684]\n","pred: [5.8416357] | real: [8.29400682]\n","pred: [5.8416357] | real: [4.65881395]\n","pred: [5.8416357] | real: [8.22546697]\n","pred: [5.8416357] | real: [3.43993115]\n","pred: [5.8416357] | real: [0.22870922]\n","pred: [5.8416357] | real: [0.38993955]\n","pred: [5.8416357] | real: [0.66369367]\n","pred: [5.8416357] | real: [0.44879103]\n","pred: [5.8416357] | real: [1.94954133]\n","pred: [5.8416357] | real: [3.52844286]\n","pred: [5.8416357] | real: [2.74496722]\n","pred: [5.8416357] | real: [1.04674244]\n","pred: [5.8416357] | real: [0.91578531]\n","pred: [5.8416357] | real: [4.58082652]\n","pred: [5.8416357] | real: [2.48695683]\n","pred: [5.8416357] | real: [2.96786165]\n","pred: [5.8416357] | real: [2.65296745]\n","pred: [5.8416357] | real: [1.00142026]\n","pred: [5.8416357] | real: [3.11701059]\n","pred: [5.8416357] | real: [2.60095286]\n","pred: [5.8416357] | real: [8.12490821]\n","pred: [5.8416357] | real: [0.88722205]\n","pred: [5.8416357] | real: [2.55114961]\n","pred: [5.8416357] | real: [2.63708067]\n","pred: [5.8416357] | real: [1.05382466]\n","pred: [5.8416357] | real: [4.17333102]\n","pred: [5.8416357] | real: [3.41364813]\n","pred: [5.8416357] | real: [3.37400842]\n","pred: [5.8416357] | real: [12.1985414]\n"]}],"source":["from keras.callbacks import TensorBoard\n","import os\n","import datetime\n","\n","# log\n","log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","\n","def add_hidden_layer(model, units, activation, layers):\n","\n","    if layers > 1:\n","\n","        for layer in range(layers - 1):\n","\n","            model.add(LSTM(units=units, activation=activation, return_sequences=True))\n","\n","    model.add(LSTM(units=units, activation=activation, return_sequences=False))\n","\n","\n","# loop params\n","\n","LSTM_units = 50\n","\n","Stop_patience = 10\n","\n","Batch_size = 64\n","\n","Learning_rate = 1e-2\n","\n","Epochs = 200\n","\n","Layers = 0\n","\n","Activation = \"sigmoid\"\n","\n","\n","# loss_function = 'mean_absolute_error'  # Assuming you are doing regression\n","\n","# loss_function = 'categorical_crossentropy'\n","\n","# loss_function = 'sparse_categorical_crossentropy'\n","\n","loss_function = 'mean_squared_error'\n","\n","\n","# Initialize LossHistory with validation data\n","\n","history = LossHistory(x_train_sequences, y_train_sequences, x_val_sequences, y_val_sequences)\n","\n","\n","def model_fit():\n","\n","\n","    print(\"epochs:\", Epochs)\n","\n","    print(\"batch:\", Batch_size)\n","\n","    print(\"units:\", LSTM_units)\n","\n","\n","    # Build LSTM model\n","\n","    model = Sequential()\n","\n","    # Adding LSTM layer with L2 regularization\n","    model.add(Input(shape=(x_train_sequences.shape[1], x_train_sequences.shape[2])))\n","\n","    model.add(LSTM(units=LSTM_units, activation=Activation, return_sequences=False))  # Regularization on the weights\n","\n","\n","    # add_hidden_layer(model, LSTM_units, 'sigmoid', Layers)\n","\n","    print(\"hidden layers:\", Layers)\n","\n","\n","    # full con layer\n","\n","    # model.add(Dense(units=32))\n","\n","    model.add(Dense(units=1))\n","\n","\n","    # Compile model\n","\n","    model.compile(optimizer=Adam(learning_rate=Learning_rate), loss=loss_function, metrics=['mean_absolute_error'])\n","\n","\n","    # Define early stopping\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=Stop_patience, restore_best_weights=True)\n","\n","    # early_stopping = EarlyStopping(monitor='val_mean_absolute_error', patience=Stop_patience, restore_best_weights=True)\n","\n","\n","    # Train the model with early stopping\n","\n","    model.fit(x_train_sequences, y_train_sequences, epochs=Epochs, batch_size=Batch_size,\n","\n","            validation_data=(x_val_sequences, y_val_sequences), callbacks=[history, early_stopping, tensorboard_callback])\n","\n","\n","    prediction_results = model.predict(x_test_sequences)\n","\n","\n","    _abs = np.abs(y_test_sequences - prediction_results)\n","\n","    mae = np.mean(_abs)\n","\n","\n","    print(\"MAE:\", mae)\n","\n","\n","    for i in range(40):\n","\n","        print(f\"pred: {prediction_results[i]} | real: {y_test_sequences[i]}\")\n","\n","\n","model_fit()\n","\n","\n","# MSE\n","\n","# mse = mean_squared_error(y_test, prediction_results)\n","\n","# print(\"Mean Squared Error:\", mse)\n","\n","\n","# RMSE\n","\n","# rmse = np.sqrt(mse)\n","\n","# print(\"Mean Squared Error:\", mse)\n","\n","\n","# MAE\n","\n","# mae = mean_absolute_error(y_test_sequences, prediction_results)\n","\n","# print(\"Mean Absoluted Error:\", mae)\n","\n","\n","# from sklearn.metrics import mean_squared_error\n","\n","\n","# mse = mean_squared_error(y_test_sequences, prediction_results)\n","\n","# print(\"Mean Squared Error:\", mse)"]},{"cell_type":"markdown","metadata":{"id":"a53Vs3uAibih"},"source":["### SAVE MODEL"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1715196212518,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"O1YJC_q-G-JF"},"outputs":[],"source":["# Save the model\n","    # model.save(\"/content/drive/MyDrive/LSTM/predict_model_2_0v.keras\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNCkyDBPwpKDBZmdmhechB3","gpuType":"T4","mount_file_id":"1NLGgGq5VOK54JvoBCH_NFc1AcJU7dBFP","provenance":[{"file_id":"1IoRKvFLnhdaROtNT4BDeOHAptBELr5wc","timestamp":1713973444923},{"file_id":"1oKKWNRjGFCyRAq7WE_FI6_Dw1ovcQjRn","timestamp":1708701671512}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
