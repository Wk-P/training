{"cells":[{"cell_type":"markdown","metadata":{"id":"qgtH9_K1imY7"},"source":["### DATA PREPROGRESS"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":490,"status":"ok","timestamp":1715196153067,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"wHuwHZf_jw9N"},"outputs":[],"source":["import re\n","import numpy as np\n","import json\n","\n","def data_preprocess(filepath):\n","    # data_all = np.genfromtxt(filepath, delimiter=' ')\n","    # data = data_all[:8000]\n","\n","    data = np.genfromtxt(filepath, delimiter=' ', dtype=float)\n","\n","    for d in data:\n","        # d[0] = float(d[0] * 1000)\n","        # d[2] = float(d[2] * 1000)\n","        # d[3] = float(d[2] * 1000)\n","        # d[4] = float(d[2] * 1000)\n","        pass\n","\n","    training_x = data[:, 1:]\n","\n","    training_y = data[:, :1]\n","\n","    return training_x, training_y\n","\n","# Data preprocessing\n","file_path = \"../training_data/trainingDataSet_CPU_v1_10.txt\"\n","training_x, training_y = data_preprocess(file_path)"]},{"cell_type":"markdown","metadata":{"id":"ct0ytUeniiq0"},"source":["### MODEL COMPILE & FIT"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1715196153561,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"q0bCz2Ma4sQu"},"outputs":[],"source":["### Data preprocessing using MinMaxScaler\n","# create MinMaxScaler Object\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","x_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()\n","\n","\n","# scaled_training_x = training_x[:, 0].reshape(-1, 1)\n","# scaled_training_y = training_y[:].reshape(-1, 1)\n","\n","# training_x[:, 0] = scaled_training_x[:, 0]\n","# training_y = scaled_training_y\n","\n","# Define the split points based on time\n","train_size = int(len(training_x) * 0.8)\n","val_size = int(len(training_x) * 0.1)\n","\n","# Split the data\n","x_train = training_x[:train_size]\n","y_train = training_y[:train_size]\n","\n","x_val = training_x[train_size + val_size:]\n","y_val = training_y[train_size + val_size:]\n","\n","\n","# test data from other file\n","x_test = training_x[:train_size + val_size]\n","y_test = training_y[:train_size + val_size]\n","\n","# x_train_first_column = x_train[:, 0].reshape(-1, 1)\n","# scaled_x_train_first_column = x_scaler.fit_transform(x_train_first_column)\n","# x_train[:, 0] = scaled_x_train_first_column.flatten()\n","# y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))  # reshap to 2D\n","\n","# make the same sacling on valid data set and test data set\n","# x_val_scaled = x_scaler.transform(x_val)\n","# x_val_first_column = x_val[:, 0].reshape(-1, 1)\n","# scaled_x_val_first_column = x_scaler.fit_transform(x_val_first_column)\n","# x_val[:, 0] = scaled_x_val_first_column.flatten()\n","# y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1))  # reshap to 2D\n","\n","\n","# x_test_first_column = x_test[:, 0].reshape(-1, 1)\n","# scaled_x_test_first_column = x_scaler.fit_transform(x_test_first_column)\n","# x_test[:, 0] = scaled_x_test_first_column.flatten()\n","# y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))  # reshap to 2D"]},{"cell_type":"markdown","metadata":{"id":"B1eEvGJzL186"},"source":["### NORMALIZATION"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":342,"status":"ok","timestamp":1715196153902,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"VGTTcyJALYSX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout, Input\n","from keras.callbacks import Callback, EarlyStopping\n","from keras.optimizers import Adam\n","\n","\n","def create_sequences(dataset_x, dataset_y, time_steps):\n","    dataX, dataY = [], []\n","\n","    # scale for data set\n","    if dataset_x.shape[0] < time_steps:\n","        # Handle the case where the input data is smaller than time_steps\n","        # You can choose to pad the sequence or handle it in a way that makes sense for your data\n","        # For example, you can replicate the single time step to create a sequence\n","        x_sequence = np.tile(dataset_x, (time_steps, 1))\n","        y_sequence = np.tile(dataset_y, (time_steps, 1))\n","        dataX.append(x_sequence)\n","        dataY.append(y_sequence)\n","    else:\n","        for i in range(dataset_x.shape[0] - time_steps + 1):\n","            x_sequence = dataset_x[i:i + time_steps, :]\n","            y_sequence = dataset_y[i + time_steps - 1]\n","            dataX.append(x_sequence)\n","            dataY.append(y_sequence)\n","\n","    return np.array(dataX), np.array(dataY)\n","\n","# Create sequences for each set\n","time_steps = 50\n","\n","\n","# x_test = training_x[train_size:]\n","# y_test = one_hot_encoding[train_size:]\n","\n","# Create sequences for each set\n","x_train_sequences, y_train_sequences = create_sequences(x_train, y_train, time_steps)\n","x_val_sequences, y_val_sequences = create_sequences(x_val, y_val, time_steps)\n","x_test_sequences, y_test_sequences = create_sequences(x_test, y_test, time_steps)\n","\n","\n","# Initialize LossHistory with validation data\n","class LossHistory(Callback):\n","    def __init__(self, x_train, y_train, x_val, y_val):\n","        super().__init__()\n","        self.train_data = (x_train, y_train)\n","        self.validation_data = (x_val, y_val)\n","        self.losses = []\n","        self.val_losses = []\n","        self.train_errors = []\n","        self.val_errors = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.losses.append(logs['loss'])\n","        self.val_losses.append(logs.get('val_loss'))\n","        self.train_errors.append(logs.get('mean_absolute_error'))\n","        self.val_errors.append(logs.get('val_mean_absolute_error'))"]},{"cell_type":"markdown","metadata":{"id":"k_HW6eWMLZMX"},"source":["### FITTING"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":58616,"status":"error","timestamp":1715196212517,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"F3ZXcO-_j4eM","outputId":"f89bb4c1-94d0-4916-da3d-f77068c91b52"},"outputs":[{"name":"stdout","output_type":"stream","text":["epochs: 200\n","batch: 64\n","units: 50\n","hidden layers: 0\n","Epoch 1/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 3381610.5000 - mean_absolute_error: 113.8002 - val_loss: 55.1970 - val_mean_absolute_error: 4.9078\n","Epoch 2/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 54.2562 - mean_absolute_error: 4.7985 - val_loss: 45.2229 - val_mean_absolute_error: 4.1853\n","Epoch 3/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 43.2047 - mean_absolute_error: 4.0999 - val_loss: 37.4512 - val_mean_absolute_error: 3.8681\n","Epoch 4/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 37.7986 - mean_absolute_error: 3.8724 - val_loss: 33.4741 - val_mean_absolute_error: 3.9105\n","Epoch 5/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 33.2920 - mean_absolute_error: 3.9133 - val_loss: 32.1992 - val_mean_absolute_error: 4.0581\n","Epoch 6/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 31.9595 - mean_absolute_error: 4.0179 - val_loss: 31.9909 - val_mean_absolute_error: 4.1588\n","Epoch 7/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 32.1705 - mean_absolute_error: 4.1213 - val_loss: 31.9843 - val_mean_absolute_error: 4.1980\n","Epoch 8/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 32.9731 - mean_absolute_error: 4.1840 - val_loss: 31.9861 - val_mean_absolute_error: 4.2038\n","Epoch 9/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 32.6587 - mean_absolute_error: 4.1520 - val_loss: 31.9894 - val_mean_absolute_error: 4.2110\n","Epoch 10/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 32.1846 - mean_absolute_error: 4.1436 - val_loss: 31.9866 - val_mean_absolute_error: 4.2049\n","Epoch 11/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 32.3450 - mean_absolute_error: 4.1617 - val_loss: 31.9906 - val_mean_absolute_error: 4.2133\n","Epoch 12/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 31.8542 - mean_absolute_error: 4.1154 - val_loss: 31.9860 - val_mean_absolute_error: 4.2034\n","Epoch 13/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 32.5432 - mean_absolute_error: 4.1665 - val_loss: 31.9957 - val_mean_absolute_error: 4.2211\n","Epoch 14/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 32.4612 - mean_absolute_error: 4.1886 - val_loss: 31.9915 - val_mean_absolute_error: 4.1578\n","Epoch 15/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 32.7527 - mean_absolute_error: 4.1535 - val_loss: 31.9969 - val_mean_absolute_error: 4.1505\n","Epoch 16/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 32.6306 - mean_absolute_error: 4.1558 - val_loss: 31.9944 - val_mean_absolute_error: 4.2193\n","Epoch 17/200\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 33.1000 - mean_absolute_error: 4.2050 - val_loss: 31.9895 - val_mean_absolute_error: 4.2113\n","\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n"]},{"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (45000,1) (44951,50,1) ","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[36], line 119\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m):\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_results[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | real: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test_sequences[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 119\u001b[0m \u001b[43mmodel_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# MSE\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# mse = mean_squared_error(y_test, prediction_results)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# print(\"Mean Squared Error:\", mse)\u001b[39;00m\n","Cell \u001b[1;32mIn[36], line 106\u001b[0m, in \u001b[0;36mmodel_fit\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train_sequences, y_train_sequences, epochs\u001b[38;5;241m=\u001b[39mEpochs, batch_size\u001b[38;5;241m=\u001b[39mBatch_size,\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(x_val_sequences, y_val_sequences), callbacks\u001b[38;5;241m=\u001b[39m[history, early_stopping, tensorboard_callback])\n\u001b[0;32m    103\u001b[0m prediction_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test_sequences)\n\u001b[1;32m--> 106\u001b[0m _abs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43my_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprediction_results\u001b[49m)\n\u001b[0;32m    108\u001b[0m mae \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(_abs)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mae)\n","\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (45000,1) (44951,50,1) "]}],"source":["from keras.callbacks import TensorBoard\n","import os\n","import datetime\n","\n","# log\n","log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","\n","def add_hidden_layer(model, units, activation, layers):\n","\n","    if layers > 1:\n","\n","        for layer in range(layers - 1):\n","\n","            model.add(LSTM(units=units, activation=activation, return_sequences=True))\n","\n","    model.add(LSTM(units=units, activation=activation, return_sequences=False))\n","\n","\n","# loop params\n","\n","LSTM_units = 50\n","\n","Stop_patience = 10\n","\n","Batch_size = 64\n","\n","Learning_rate = 1e-2\n","\n","Epochs = 200\n","\n","Layers = 0\n","\n","Activation = \"relu\"\n","\n","\n","# loss_function = 'mean_absolute_error'  # Assuming you are doing regression\n","\n","# loss_function = 'categorical_crossentropy'\n","\n","# loss_function = 'sparse_categorical_crossentropy'\n","\n","loss_function = 'mean_squared_error'\n","\n","\n","# Initialize LossHistory with validation data\n","\n","history = LossHistory(x_train_sequences, y_train_sequences, x_val_sequences, y_val_sequences)\n","\n","\n","def model_fit():\n","\n","\n","    print(\"epochs:\", Epochs)\n","\n","    print(\"batch:\", Batch_size)\n","\n","    print(\"units:\", LSTM_units)\n","\n","\n","    # Build LSTM model\n","\n","    model = Sequential()\n","\n","    # Adding LSTM layer with L2 regularization\n","    model.add(Input(shape=(x_train_sequences.shape[1], x_train_sequences.shape[2])))\n","\n","    model.add(LSTM(units=LSTM_units, activation=Activation, return_sequences=False))  # Regularization on the weights\n","\n","\n","    # add_hidden_layer(model, LSTM_units, 'sigmoid', Layers)\n","\n","    print(\"hidden layers:\", Layers)\n","\n","\n","    # full con layer\n","\n","    model.add(Dense(units=32))\n","\n","    model.add(Dense(units=1))\n","\n","\n","    # Compile model\n","\n","    model.compile(optimizer=Adam(learning_rate=Learning_rate), loss=loss_function, metrics=['mean_absolute_error'])\n","\n","\n","    # Define early stopping\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=Stop_patience, restore_best_weights=True)\n","\n","    # early_stopping = EarlyStopping(monitor='val_mean_absolute_error', patience=Stop_patience, restore_best_weights=True)\n","\n","\n","    # Train the model with early stopping\n","\n","    model.fit(x_train_sequences, y_train_sequences, epochs=Epochs, batch_size=Batch_size,\n","\n","            validation_data=(x_val_sequences, y_val_sequences), callbacks=[history, early_stopping, tensorboard_callback])\n","\n","\n","    prediction_results = model.predict(x_test_sequences)\n","\n","\n","    _abs = np.abs(y_test_sequences - prediction_results)\n","\n","    mae = np.mean(_abs)\n","\n","\n","    print(\"MAE:\", mae)\n","\n","\n","    for i in range(40):\n","\n","        print(f\"pred: {prediction_results[i]} | real: {y_test_sequences[i]}\")\n","\n","\n","model_fit()\n","\n","\n","# MSE\n","\n","# mse = mean_squared_error(y_test, prediction_results)\n","\n","# print(\"Mean Squared Error:\", mse)\n","\n","\n","# RMSE\n","\n","# rmse = np.sqrt(mse)\n","\n","# print(\"Mean Squared Error:\", mse)\n","\n","\n","# MAE\n","\n","# mae = mean_absolute_error(y_test_sequences, prediction_results)\n","\n","# print(\"Mean Absoluted Error:\", mae)\n","\n","\n","# from sklearn.metrics import mean_squared_error\n","\n","\n","# mse = mean_squared_error(y_test_sequences, prediction_results)\n","\n","# print(\"Mean Squared Error:\", mse)"]},{"cell_type":"markdown","metadata":{"id":"a53Vs3uAibih"},"source":["### SAVE MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1715196212518,"user":{"displayName":"LINGXIAO ZHOU","userId":"00374870973427248310"},"user_tz":-540},"id":"O1YJC_q-G-JF"},"outputs":[],"source":["# Save the model\n","    # model.save(\"/content/drive/MyDrive/LSTM/predict_model_2_0v.keras\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNCkyDBPwpKDBZmdmhechB3","gpuType":"T4","mount_file_id":"1NLGgGq5VOK54JvoBCH_NFc1AcJU7dBFP","provenance":[{"file_id":"1IoRKvFLnhdaROtNT4BDeOHAptBELr5wc","timestamp":1713973444923},{"file_id":"1oKKWNRjGFCyRAq7WE_FI6_Dw1ovcQjRn","timestamp":1708701671512}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
