{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openpyxl import Workbook, load_workbook  # type: ignore\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def to_excel(\n",
    "    data, filename, sheet_style, result_dir_path, new_sheet=False, sheet_name=\"Sheet1\"\n",
    "):\n",
    "\n",
    "    if not os.path.exists(result_dir_path):\n",
    "        os.makedirs(result_dir_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(result_dir_path, f\"{filename}.xlsx\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        workbook = load_workbook(file_path)\n",
    "        if new_sheet:\n",
    "            sheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "        else:\n",
    "            sheet = workbook.active\n",
    "    else:\n",
    "        workbook = Workbook()\n",
    "        default_sheet = workbook.active\n",
    "        workbook.remove(default_sheet)\n",
    "        sheet = workbook.create_sheet(title=sheet_name)\n",
    "        \n",
    "    if sheet_style:\n",
    "        sheet.append(sheet_style)\n",
    "\n",
    "\n",
    "    if new_sheet:\n",
    "        print(data)\n",
    "    # write data into file\n",
    "    for row in data:\n",
    "        sheet.append(row)\n",
    "\n",
    "    workbook.save(filename=f\"{result_dir_path}\\\\{filename}.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "# read input dataset\n",
    "filename_prefix = (\n",
    "    \"RandomRequestNumberclientv_single_worker_node#loops3#requests_batch150#Thu-Aug-22-22-08-38-2024\"\n",
    ")\n",
    "\n",
    "\n",
    "dataset_read_filename = filename_prefix\n",
    "training_data_dir = Path.cwd().parent / \"training_data\" / \"data_set8\"\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "file_path = f\"{training_data_dir}\\\\{dataset_read_filename}.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "# set result output filename and path\n",
    "result_suffix = \"result\"\n",
    "\n",
    "result_dir_path = Path.cwd().parent / \"results\" / \"result_processTime_waitTasks_v4\"\n",
    "\n",
    "if not os.path.exists(result_dir_path):\n",
    "    os.makedirs(result_dir_path)\n",
    "\n",
    "version_index = len([_ for _ in Path(result_dir_path).iterdir() if _.is_file()])\n",
    "version = f\"_v{version_index}\"\n",
    "\n",
    "result_name = \"processTime#waitTasks\" + version\n",
    "result_output_filename = f\"{filename_prefix}{result_name}{result_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    columns = df.columns.to_list()\n",
    "    data_dict = {col: df[col].to_list() for col in columns}\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def data_preprocess(filepath):\n",
    "    data = read_data(filepath)\n",
    "    # TODO more...\n",
    "\n",
    "    # to numpy\n",
    "    for key in data.keys():\n",
    "        data[key] = np.array(data[key])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_preprocess(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Style View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# # dataset is a dictionary\n",
    "\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import cupy as cp\n",
    "\n",
    "y = cp.array(dataset.get(\"processed_and_waited_time_on_manager_node\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strandard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler       # type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "\n",
    "_X = [dataset.get(\"request_num\"), dataset.get(\"sub_processing_tasks_on_manager_node\"), dataset.get(\"processed_time_on_worker_node\")]\n",
    "\n",
    "for i in range(len(_X)):\n",
    "    _X[i] = np.array(_X[i]).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "# _scaler = \"normalization\"\n",
    "_scaler = None\n",
    "\n",
    "if _scaler == \"standard\":\n",
    "    X_scaler = StandardScaler()\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "elif _scaler == \"normalization\":\n",
    "    X_scaler = MinMaxScaler()\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "elif _scaler == \"log\":\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = np.log1p(_X[i])\n",
    "elif _scaler == \"power\":\n",
    "    X_scaler = PowerTransformer(method='yeo-johnson')\n",
    "    for i in range(len(_X)):\n",
    "        _X[i] = X_scaler.fit_transform(_X[i])\n",
    "else:\n",
    "    pass\n",
    "\n",
    "for i in range(len(_X)):\n",
    "    _X[i] = _X[i].reshape(-1)\n",
    "\n",
    "X = cp.asarray(np.array(\n",
    "    _X\n",
    ").T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter combinations: 2\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the parameter ranges\n",
    "params = {\n",
    "    \"max_depth\": [5],\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"min_child_weight\": [1],\n",
    "    \"subsample\": [0.1],\n",
    "    \"colsample_bytree\": [0.1],\n",
    "    \"reg_alpha\": [0.1],\n",
    "    \"reg_lambda\": [0.1],\n",
    "}\n",
    "\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "parameter_combinations = [\n",
    "    dict(zip(params.keys(), combination))\n",
    "    for combination in itertools.product(*params.values())\n",
    "]\n",
    "\n",
    "# Print the number of parameter combinations and a few examples\n",
    "print(f\"Number of parameter combinations: {len(parameter_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor  # type: ignore\n",
    "import xgboost  # type: ignore\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error  # type: ignore\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# Create the XGBoost regressor model\n",
    "def create_model(params):\n",
    "    # if use gpu to add these two params\n",
    "    # - tree_method=\"hist\",\n",
    "    # - device=\"cuda\",\n",
    "    \n",
    "    return XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        reg_alpha=params[\"reg_alpha\"],\n",
    "        reg_lambda=params[\"reg_lambda\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(\n",
    "    params, X_train, y_train, X_test: np.array, y_test: np.array\n",
    ") -> Tuple[np.float64, xgboost.Booster]:\n",
    "\n",
    "    model = create_model(params)\n",
    "\n",
    "    # Convert CuPy arrays to NumPy arrays\n",
    "\n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "    # typecasting  cp -> np\n",
    "    y_test_np = cp.asnumpy(y_test)\n",
    "    X_test_np = cp.asnumpy(X_test)\n",
    "    predictions_np = cp.asnumpy(predictions)\n",
    "\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = float(mean_squared_error(y_test_np, predictions_np))\n",
    "    mae = float(mean_absolute_error(y_test_np, predictions_np))\n",
    "    avg = float(np.mean(y_test_np))\n",
    "    avg_acc = 100 * (avg - mae) / avg\n",
    "\n",
    "    # calculate diffirence and accuracy\n",
    "    data_list = list()\n",
    "    for i in range(len(y_test_np)):\n",
    "        acc = 0\n",
    "        diff = abs(y_test_np[i] - predictions_np[i])\n",
    "        if predictions[i] < 0:\n",
    "            acc = 0\n",
    "        else:\n",
    "            rate = diff / y_test_np[i]\n",
    "            if rate < 1 and rate >= 0:\n",
    "                acc = 1 - rate\n",
    "            if rate > 1:\n",
    "                acc = 0\n",
    "        data_list.append(\n",
    "            [\n",
    "                float(X_test_np[i][0]),\n",
    "                float(X_test_np[i][1]),\n",
    "                float(X_test_np[i][2]),\n",
    "                float(y_test_np[i]),\n",
    "                float(predictions_np[i]),\n",
    "                float(diff),\n",
    "                float(round(acc, 5)),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    results_tuple = {\"mse\": mse, \"mae\": mae, \"avg\": avg, \"avg_acc\": avg_acc, \"model\": model, \"data_list\": data_list}\n",
    "    filtered_results = {k: v for k, v in results_tuple.items() if k != \"model\" and k != \"data_list\"}\n",
    "    print(filtered_results, f'\\n{params}', '\\n')\n",
    "    return results_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 558.5609927744022, 'mae': 20.41687072912852, 'avg': 53.12526133590274, 'avg_acc': 61.56843238843414} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 200, 'min_child_weight': 1, 'subsample': 0.1, 'colsample_bytree': 0.1, 'reg_alpha': 0.1, 'reg_lambda': 0.1} \n",
      "\n",
      "Best acc 61.56843238843414\n",
      "{'mse': 516.4825417899154, 'mae': 19.014944619602627, 'avg': 46.475114422374304, 'avg_acc': 59.085749748152644} \n",
      "{'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 400, 'min_child_weight': 1, 'subsample': 0.1, 'colsample_bytree': 0.1, 'reg_alpha': 0.1, 'reg_lambda': 0.1} \n",
      "\n",
      "Best acc 61.56843238843414\n",
      "Best Parameters: {'max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 400, 'min_child_weight': 1, 'subsample': 0.1, 'colsample_bytree': 0.1, 'reg_alpha': 0.1, 'reg_lambda': 0.1}\n",
      "Best MSE: 516.4825417899154\n",
      "Best MAE: 19.014944619602627\n",
      "Average: 46.475114422374304\n",
      "Average accuracy: 59.085749748152644\n",
      "Results saved finished.\n",
      "[[5, 0.01, 400, 1, 0.1, 0.1, 0.1, 0.1]]\n",
      "[[5, 0.01, 200, 1, 0.1, 0.1, 0.1, 0.1]]\n",
      "[[5, 0.01, 400, 1, 0.1, 0.1, 0.1, 0.1]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track the best parameters\n",
    "best_params = None\n",
    "best_result = None\n",
    "\n",
    "# results param\n",
    "best_mse = float(\"inf\")\n",
    "best_avg_acc = float(-1)\n",
    "best_mae = float(\"inf\")\n",
    "\n",
    "# result params tuple\n",
    "best_avg_acc_params = None\n",
    "best_mae_params = None\n",
    "\n",
    "\n",
    "# control sheet style of excel written\n",
    "first_write = True\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate each parameter combination\n",
    "for params in parameter_combinations:\n",
    "\n",
    "    # split dataset to train dataset and test dataset for every params group\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=None\n",
    "    )\n",
    "    results_tuple = evaluate_model(params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    filtered_results = {\n",
    "        k: v for k, v in results_tuple.items() if k != \"model\" and k != \"data_list\"\n",
    "    }\n",
    "\n",
    "    # update best model\n",
    "    if results_tuple['mse'] < best_mse:\n",
    "        best_mse = results_tuple[\"mse\"]\n",
    "        best_params = params\n",
    "        best_model = results_tuple[\"model\"]\n",
    "        best_result = filtered_results\n",
    "\n",
    "    # write result into excel file\n",
    "    sheet_style = [\"num\", \"jobs\", \"processed_time\", \"X_test\", \"prediction\", \"difference\", \"accuracy\"]\n",
    "    avg_acc = results_tuple[\"avg_acc\"]\n",
    "    data_list = results_tuple['data_list']\n",
    "\n",
    "    if best_avg_acc < results_tuple['avg_acc']:\n",
    "        best_avg_acc = results_tuple[\"avg_acc\"]\n",
    "        best_acc_params = params\n",
    "\n",
    "    if best_mae > results_tuple['mae']:\n",
    "        best_mae = results_tuple[\"mae\"]\n",
    "        best_mae_params = params\n",
    "\n",
    "    if first_write:\n",
    "        # sheet 1\n",
    "        to_excel(\n",
    "            data=data_list,\n",
    "            filename=result_output_filename,\n",
    "            sheet_style=sheet_style,\n",
    "            result_dir_path=result_dir_path,\n",
    "            new_sheet=False,\n",
    "        )\n",
    "        first_write = False\n",
    "    else:\n",
    "        to_excel(\n",
    "            data=data_list,\n",
    "            filename=result_output_filename,\n",
    "            sheet_style=None,\n",
    "            result_dir_path=result_dir_path,\n",
    "            new_sheet=False,\n",
    "        )\n",
    "    print(f\"Best acc {best_avg_acc}\")\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE:\", best_mse)\n",
    "print(\"Best MAE:\", best_result[\"mae\"])\n",
    "print(\"Average:\", best_result[\"avg\"])\n",
    "print(\"Average accuracy:\", best_result[\"avg_acc\"])\n",
    "print(\"Results saved finished.\")\n",
    "\n",
    "params_keys_list = list(best_params.keys())\n",
    "params_values_list = list(best_params.values())\n",
    "\n",
    "# best mse params\n",
    "to_excel(\n",
    "    data=[params_values_list],\n",
    "    filename=result_output_filename,\n",
    "    sheet_style=params_keys_list,\n",
    "    result_dir_path=result_dir_path,\n",
    "    new_sheet=True,\n",
    "    sheet_name=\"best mse params\",\n",
    ")\n",
    "\n",
    "\n",
    "result_keys_list = list(filtered_results.keys())\n",
    "\n",
    "# avg_acc sheet\n",
    "to_excel(\n",
    "    data=[list(best_acc_params.values())],\n",
    "    filename=result_output_filename,\n",
    "    sheet_style=result_keys_list,\n",
    "    result_dir_path=result_dir_path,\n",
    "    new_sheet=True,\n",
    "    sheet_name=\"best avg acc params\",\n",
    ")\n",
    "\n",
    "\n",
    "# mae sheet\n",
    "to_excel(\n",
    "    data=[list(best_mae_params.values())],\n",
    "    filename=result_output_filename,\n",
    "    sheet_style=result_keys_list,\n",
    "    result_dir_path=result_dir_path,\n",
    "    new_sheet=True,\n",
    "    sheet_name=\"best mae params\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# model_file = \"xboost_tasks_time\" + \".json\"\n",
    "# model_path = str(Path.cwd() / \"modelsfile\" / model_file)\n",
    "# best_model.save_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
